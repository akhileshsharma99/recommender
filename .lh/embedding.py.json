{
    "sourceFile": "embedding.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 10,
            "patches": [
                {
                    "date": 1678412088305,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1678412896105,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -38,4 +38,28 @@\n query = \"200g of lean beef how many calories\"\r\n result = qa({\"query\": query})\r\n print(result['result'])\r\n print(result['source_documents'])\r\n+\r\n+\r\n+import openai\r\n+from transformers import GPT2Tokenizer\r\n+\r\n+\r\n+class Gpt:\r\n+\r\n+    def __init__(self, api_key: str) -> None:\r\n+        openai.api_key = api_key\r\n+        self._model = model\r\n+        self._max_tokens = max_tokens\r\n+        self._tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\r\n+\r\n+    def generate_response(self, prompt) -> str:\r\n+        response = openai.Completion.create(\r\n+            model=self._model, prompt=prompt, temperature=0, top_p=0, max_tokens=self._get_max_tokens(prompt=prompt))\r\n+        text = response['choices'][0]['text']\r\n+        return text\r\n+\r\n+    def _get_max_tokens(self, prompt):\r\n+        # Tokenize the input text\r\n+        input_tokens = len(self._tokenizer(prompt)['input_ids'])\r\n+        return self._max_tokens - input_tokens\r\n"
                },
                {
                    "date": 1678412913170,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -47,12 +47,10 @@\n \r\n class Gpt:\r\n \r\n     def __init__(self, api_key: str) -> None:\r\n-        openai.api_key = api_key\r\n-        self._model = model\r\n-        self._max_tokens = max_tokens\r\n-        self._tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\r\n+        self.api_key = api_key\r\n+        \r\n \r\n     def generate_response(self, prompt) -> str:\r\n         response = openai.Completion.create(\r\n             model=self._model, prompt=prompt, temperature=0, top_p=0, max_tokens=self._get_max_tokens(prompt=prompt))\r\n"
                },
                {
                    "date": 1678413031065,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -4,8 +4,13 @@\n from langchain import OpenAI, VectorDBQA\r\n from langchain.document_loaders import DirectoryLoader\r\n import os\r\n \r\n+class LLM:\r\n+\r\n+    def __init__(self, api_key: str) -> None:\r\n+        self.api_key = api_key\r\n+\r\n # Set the directories for the dataset and the persisted vector database\r\n datasets_directory = 'nutrition/datasets/kaggle_food_nutrition'\r\n persist_directory = 'nutrition/db/kaggle_food_nutrition'\r\n \r\n@@ -40,24 +45,7 @@\n print(result['result'])\r\n print(result['source_documents'])\r\n \r\n \r\n-import openai\r\n-from transformers import GPT2Tokenizer\r\n \r\n+    \r\n \r\n-class Gpt:\r\n-\r\n-    def __init__(self, api_key: str) -> None:\r\n-        self.api_key = api_key\r\n-        \r\n-\r\n-    def generate_response(self, prompt) -> str:\r\n-        response = openai.Completion.create(\r\n-            model=self._model, prompt=prompt, temperature=0, top_p=0, max_tokens=self._get_max_tokens(prompt=prompt))\r\n-        text = response['choices'][0]['text']\r\n-        return text\r\n-\r\n-    def _get_max_tokens(self, prompt):\r\n-        # Tokenize the input text\r\n-        input_tokens = len(self._tokenizer(prompt)['input_ids'])\r\n-        return self._max_tokens - input_tokens\r\n"
                },
                {
                    "date": 1678413091737,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -18,9 +18,9 @@\n try:\r\n     embeddings = OpenAIEmbeddings(openai_api_key=os.environ['OPENAI_API_KEY'])\r\n except KeyError:\r\n     print(\"Error: OPENAI_API_KEY environment variable not set\")\r\n-    exit()\r\n+    raise\r\n \r\n # If the vector database has already been created, load it from disk\r\n if os.path.exists(persist_directory):\r\n     vectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\r\n"
                },
                {
                    "date": 1678413189443,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -9,19 +9,18 @@\n \r\n     def __init__(self, api_key: str) -> None:\r\n         self.api_key = api_key\r\n \r\n+    def embedding(self):\r\n+        # Create an instance of OpenAIEmbeddings using the API \r\n+        embeddings = OpenAIEmbeddings(openai_api_key=self.api_key)\r\n+\r\n # Set the directories for the dataset and the persisted vector database\r\n datasets_directory = 'nutrition/datasets/kaggle_food_nutrition'\r\n persist_directory = 'nutrition/db/kaggle_food_nutrition'\r\n \r\n-# Create an instance of OpenAIEmbeddings using the API key from the environment variable\r\n-try:\r\n-    embeddings = OpenAIEmbeddings(openai_api_key=os.environ['OPENAI_API_KEY'])\r\n-except KeyError:\r\n-    print(\"Error: OPENAI_API_KEY environment variable not set\")\r\n-    raise\r\n \r\n+\r\n # If the vector database has already been created, load it from disk\r\n if os.path.exists(persist_directory):\r\n     vectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\r\n else:\r\n"
                },
                {
                    "date": 1678413361514,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -12,9 +12,19 @@\n \r\n     def embedding(self):\r\n         # Create an instance of OpenAIEmbeddings using the API \r\n         embeddings = OpenAIEmbeddings(openai_api_key=self.api_key)\r\n+        return embeddings\r\n+    \r\n+    def createEmbedding(self, datasets_directory: str) -> List[Documents]:\r\n+        loader = DirectoryLoader(datasets_directory, glob='**/*.txt')\r\n+        documents = loader.load()\r\n \r\n+        # Split the text into smaller chunks for better performance\r\n+        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\r\n+        texts = text_splitter.split_documents(documents)\r\n+        return texts\r\n+\r\n # Set the directories for the dataset and the persisted vector database\r\n datasets_directory = 'nutrition/datasets/kaggle_food_nutrition'\r\n persist_directory = 'nutrition/db/kaggle_food_nutrition'\r\n \r\n"
                },
                {
                    "date": 1678413521174,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,9 +1,9 @@\n from langchain.embeddings.openai import OpenAIEmbeddings\r\n from langchain.vectorstores import Chroma\r\n from langchain.text_splitter import CharacterTextSplitter\r\n from langchain import OpenAI, VectorDBQA\r\n-from langchain.document_loaders import DirectoryLoader\r\n+from langchain.document_loaders import UnstructuredFileLoader\r\n import os\r\n \r\n class LLM:\r\n \r\n@@ -14,10 +14,10 @@\n         # Create an instance of OpenAIEmbeddings using the API \r\n         embeddings = OpenAIEmbeddings(openai_api_key=self.api_key)\r\n         return embeddings\r\n     \r\n-    def createEmbedding(self, datasets_directory: str) -> List[Documents]:\r\n-        loader = DirectoryLoader(datasets_directory, glob='**/*.txt')\r\n+    def createEmbedding(self, datasets_directory: str):\r\n+        loader = UnstructuredFileLoader(datasets_directory)\r\n         documents = loader.load()\r\n \r\n         # Split the text into smaller chunks for better performance\r\n         text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\r\n"
                },
                {
                    "date": 1678413999387,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -4,8 +4,10 @@\n from langchain import OpenAI, VectorDBQA\r\n from langchain.document_loaders import UnstructuredFileLoader\r\n import os\r\n \r\n+from exceptions import PathNotDirectoryException\r\n+\r\n class LLM:\r\n \r\n     def __init__(self, api_key: str) -> None:\r\n         self.api_key = api_key\r\n@@ -14,16 +16,18 @@\n         # Create an instance of OpenAIEmbeddings using the API \r\n         embeddings = OpenAIEmbeddings(openai_api_key=self.api_key)\r\n         return embeddings\r\n     \r\n-    def createEmbedding(self, datasets_directory: str):\r\n-        loader = UnstructuredFileLoader(datasets_directory)\r\n-        documents = loader.load()\r\n+    def createEmbedding(self, dataset_path: str):\r\n+        if os.path.isdir(dataset_path):\r\n+            loader = UnstructuredFileLoader(dataset_path)\r\n+            documents = loader.load()\r\n \r\n-        # Split the text into smaller chunks for better performance\r\n-        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\r\n-        texts = text_splitter.split_documents(documents)\r\n-        return texts\r\n+            # Split the text into smaller chunks for better performance\r\n+            text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\r\n+            texts = text_splitter.split_documents(documents)\r\n+            return texts\r\n+        raise PathNotDirectoryException\r\n \r\n # Set the directories for the dataset and the persisted vector database\r\n datasets_directory = 'nutrition/datasets/kaggle_food_nutrition'\r\n persist_directory = 'nutrition/db/kaggle_food_nutrition'\r\n"
                },
                {
                    "date": 1678414007029,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -27,37 +27,37 @@\n             texts = text_splitter.split_documents(documents)\r\n             return texts\r\n         raise PathNotDirectoryException\r\n \r\n-# Set the directories for the dataset and the persisted vector database\r\n-datasets_directory = 'nutrition/datasets/kaggle_food_nutrition'\r\n-persist_directory = 'nutrition/db/kaggle_food_nutrition'\r\n+# # Set the directories for the dataset and the persisted vector database\r\n+# datasets_directory = 'nutrition/datasets/kaggle_food_nutrition'\r\n+# persist_directory = 'nutrition/db/kaggle_food_nutrition'\r\n \r\n \r\n \r\n-# If the vector database has already been created, load it from disk\r\n-if os.path.exists(persist_directory):\r\n-    vectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\r\n-else:\r\n-    # If the vector database hasn't been created, load the dataset from disk\r\n-    loader = DirectoryLoader(datasets_directory, glob='**/*.txt')\r\n-    documents = loader.load()\r\n+# # If the vector database has already been created, load it from disk\r\n+# if os.path.exists(persist_directory):\r\n+#     vectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\r\n+# else:\r\n+#     # If the vector database hasn't been created, load the dataset from disk\r\n+#     loader = DirectoryLoader(datasets_directory, glob='**/*.txt')\r\n+#     documents = loader.load()\r\n \r\n-    # Split the text into smaller chunks for better performance\r\n-    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\r\n-    texts = text_splitter.split_documents(documents)\r\n+#     # Split the text into smaller chunks for better performance\r\n+#     text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\r\n+#     texts = text_splitter.split_documents(documents)\r\n \r\n-    # Create the vector database from the documents\r\n-    vectordb = Chroma.from_documents(documents=texts, embedding=embeddings, persist_directory=persist_directory)\r\n+#     # Create the vector database from the documents\r\n+#     vectordb = Chroma.from_documents(documents=texts, embedding=embeddings, persist_directory=persist_directory)\r\n \r\n-# Create a VectorDBQA instance for answering questions\r\n-qa = VectorDBQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", vectorstore=vectordb , return_source_documents=True)\r\n+# # Create a VectorDBQA instance for answering questions\r\n+# qa = VectorDBQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", vectorstore=vectordb , return_source_documents=True)\r\n \r\n-# Ask a question and print the result and source documents\r\n-query = \"200g of lean beef how many calories\"\r\n-result = qa({\"query\": query})\r\n-print(result['result'])\r\n-print(result['source_documents'])\r\n+# # Ask a question and print the result and source documents\r\n+# query = \"200g of lean beef how many calories\"\r\n+# result = qa({\"query\": query})\r\n+# print(result['result'])\r\n+# print(result['source_documents'])\r\n \r\n \r\n \r\n     \r\n"
                },
                {
                    "date": 1678414068777,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -16,11 +16,11 @@\n         # Create an instance of OpenAIEmbeddings using the API \r\n         embeddings = OpenAIEmbeddings(openai_api_key=self.api_key)\r\n         return embeddings\r\n     \r\n-    def createEmbedding(self, dataset_path: str):\r\n-        if os.path.isdir(dataset_path):\r\n-            loader = UnstructuredFileLoader(dataset_path)\r\n+    def createEmbedding(self, datasets_directory: str):\r\n+        if os.path.isdir(datasets_directory):\r\n+            loader = UnstructuredFileLoader(datasets_directory)\r\n             documents = loader.load()\r\n \r\n             # Split the text into smaller chunks for better performance\r\n             text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\r\n"
                }
            ],
            "date": 1678412088305,
            "name": "Commit-0",
            "content": "from langchain.embeddings.openai import OpenAIEmbeddings\r\nfrom langchain.vectorstores import Chroma\r\nfrom langchain.text_splitter import CharacterTextSplitter\r\nfrom langchain import OpenAI, VectorDBQA\r\nfrom langchain.document_loaders import DirectoryLoader\r\nimport os\r\n\r\n# Set the directories for the dataset and the persisted vector database\r\ndatasets_directory = 'nutrition/datasets/kaggle_food_nutrition'\r\npersist_directory = 'nutrition/db/kaggle_food_nutrition'\r\n\r\n# Create an instance of OpenAIEmbeddings using the API key from the environment variable\r\ntry:\r\n    embeddings = OpenAIEmbeddings(openai_api_key=os.environ['OPENAI_API_KEY'])\r\nexcept KeyError:\r\n    print(\"Error: OPENAI_API_KEY environment variable not set\")\r\n    exit()\r\n\r\n# If the vector database has already been created, load it from disk\r\nif os.path.exists(persist_directory):\r\n    vectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\r\nelse:\r\n    # If the vector database hasn't been created, load the dataset from disk\r\n    loader = DirectoryLoader(datasets_directory, glob='**/*.txt')\r\n    documents = loader.load()\r\n\r\n    # Split the text into smaller chunks for better performance\r\n    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\r\n    texts = text_splitter.split_documents(documents)\r\n\r\n    # Create the vector database from the documents\r\n    vectordb = Chroma.from_documents(documents=texts, embedding=embeddings, persist_directory=persist_directory)\r\n\r\n# Create a VectorDBQA instance for answering questions\r\nqa = VectorDBQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", vectorstore=vectordb , return_source_documents=True)\r\n\r\n# Ask a question and print the result and source documents\r\nquery = \"200g of lean beef how many calories\"\r\nresult = qa({\"query\": query})\r\nprint(result['result'])\r\nprint(result['source_documents'])\r\n"
        }
    ]
}