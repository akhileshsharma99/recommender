{
    "sourceFile": "llm.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 64,
            "patches": [
                {
                    "date": 1678414140726,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1678414166800,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -24,8 +24,9 @@\n \r\n             # Split the text into smaller chunks for better performance\r\n             text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\r\n             texts = text_splitter.split_documents(documents)\r\n+            print(texts)\r\n             return texts\r\n         raise PathNotDirectoryException\r\n \r\n # # Set the directories for the dataset and the persisted vector database\r\n"
                },
                {
                    "date": 1678414206203,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -26,9 +26,9 @@\n             text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\r\n             texts = text_splitter.split_documents(documents)\r\n             print(texts)\r\n             return texts\r\n-        raise PathNotDirectoryException\r\n+        raise PathNotDirectoryException(path=datasets_directory)\r\n \r\n # # Set the directories for the dataset and the persisted vector database\r\n # datasets_directory = 'nutrition/datasets/kaggle_food_nutrition'\r\n # persist_directory = 'nutrition/db/kaggle_food_nutrition'\r\n"
                },
                {
                    "date": 1678414415664,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -28,9 +28,17 @@\n             print(texts)\r\n             return texts\r\n         raise PathNotDirectoryException(path=datasets_directory)\r\n \r\n-# # Set the directories for the dataset and the persisted vector database\r\n+    def getFilesInDirectory(directory_path: str) -> list[str]:\r\n+        file_paths = [os.path.join(directory_path, filename) for filename in os.listdir(directory_path) filename]\r\n+        # iterate over files in that directory\r\n+        for filename in os.listdir(directory_path):\r\n+            f = \r\n+            # checking if it is a file\r\n+            if os.path.isfile(f):\r\n+                print(f)\r\n+        # # Set the directories for the dataset and the persisted vector database\r\n # datasets_directory = 'nutrition/datasets/kaggle_food_nutrition'\r\n # persist_directory = 'nutrition/db/kaggle_food_nutrition'\r\n \r\n \r\n"
                },
                {
                    "date": 1678414426393,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -29,16 +29,10 @@\n             return texts\r\n         raise PathNotDirectoryException(path=datasets_directory)\r\n \r\n     def getFilesInDirectory(directory_path: str) -> list[str]:\r\n-        file_paths = [os.path.join(directory_path, filename) for filename in os.listdir(directory_path) filename]\r\n-        # iterate over files in that directory\r\n-        for filename in os.listdir(directory_path):\r\n-            f = \r\n-            # checking if it is a file\r\n-            if os.path.isfile(f):\r\n-                print(f)\r\n-        # # Set the directories for the dataset and the persisted vector database\r\n+        return [os.path.join(directory_path, filename) for filename in os.listdir(directory_path) filename]\r\n+        \r\n # datasets_directory = 'nutrition/datasets/kaggle_food_nutrition'\r\n # persist_directory = 'nutrition/db/kaggle_food_nutrition'\r\n \r\n \r\n"
                },
                {
                    "date": 1678414431501,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -29,9 +29,9 @@\n             return texts\r\n         raise PathNotDirectoryException(path=datasets_directory)\r\n \r\n     def getFilesInDirectory(directory_path: str) -> list[str]:\r\n-        return [os.path.join(directory_path, filename) for filename in os.listdir(directory_path) filename]\r\n+        return [os.path.join(directory_path, filename) for filename in os.listdir(directory_path)]\r\n         \r\n # datasets_directory = 'nutrition/datasets/kaggle_food_nutrition'\r\n # persist_directory = 'nutrition/db/kaggle_food_nutrition'\r\n \r\n"
                },
                {
                    "date": 1678414453200,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -17,8 +17,9 @@\n         embeddings = OpenAIEmbeddings(openai_api_key=self._api_key)\r\n         return embeddings\r\n     \r\n     def createEmbedding(self, datasets_directory: str):\r\n+        print(getFile)\r\n         if os.path.isdir(datasets_directory):\r\n             loader = UnstructuredFileLoader(datasets_directory)\r\n             documents = loader.load()\r\n \r\n@@ -28,8 +29,9 @@\n             print(texts)\r\n             return texts\r\n         raise PathNotDirectoryException(path=datasets_directory)\r\n \r\n+    @staticmethod\r\n     def getFilesInDirectory(directory_path: str) -> list[str]:\r\n         return [os.path.join(directory_path, filename) for filename in os.listdir(directory_path)]\r\n         \r\n # datasets_directory = 'nutrition/datasets/kaggle_food_nutrition'\r\n"
                },
                {
                    "date": 1678414474699,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -17,9 +17,9 @@\n         embeddings = OpenAIEmbeddings(openai_api_key=self._api_key)\r\n         return embeddings\r\n     \r\n     def createEmbedding(self, datasets_directory: str):\r\n-        print(getFile)\r\n+        print(self.getFilesInDirectory(directory_path=datasets_directory))\r\n         if os.path.isdir(datasets_directory):\r\n             loader = UnstructuredFileLoader(datasets_directory)\r\n             documents = loader.load()\r\n \r\n"
                },
                {
                    "date": 1678414564906,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -17,18 +17,20 @@\n         embeddings = OpenAIEmbeddings(openai_api_key=self._api_key)\r\n         return embeddings\r\n     \r\n     def createEmbedding(self, datasets_directory: str):\r\n-        print(self.getFilesInDirectory(directory_path=datasets_directory))\r\n         if os.path.isdir(datasets_directory):\r\n-            loader = UnstructuredFileLoader(datasets_directory)\r\n-            documents = loader.load()\r\n+            \r\n+            texts = []\r\n+            file_paths = self.getFilesInDirectory(datasets_directory)\r\n+            \r\n+            for file_path in file_paths:\r\n+                loader = UnstructuredFileLoader(datasets_directory)\r\n+                documents = loader.load()\r\n \r\n-            # Split the text into smaller chunks for better performance\r\n-            text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\r\n-            texts = text_splitter.split_documents(documents)\r\n-            print(texts)\r\n-            return texts\r\n+                # Split the text into smaller chunks for better performance\r\n+                text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\r\n+                texts.append(text_splitter.split_documents(documents))\r\n         raise PathNotDirectoryException(path=datasets_directory)\r\n \r\n     @staticmethod\r\n     def getFilesInDirectory(directory_path: str) -> list[str]:\r\n"
                },
                {
                    "date": 1678414591626,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -29,8 +29,9 @@\n \r\n                 # Split the text into smaller chunks for better performance\r\n                 text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\r\n                 texts.append(text_splitter.split_documents(documents))\r\n+            print(texts)\r\n         raise PathNotDirectoryException(path=datasets_directory)\r\n \r\n     @staticmethod\r\n     def getFilesInDirectory(directory_path: str) -> list[str]:\r\n"
                },
                {
                    "date": 1678414637461,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -23,9 +23,9 @@\n             texts = []\r\n             file_paths = self.getFilesInDirectory(datasets_directory)\r\n             \r\n             for file_path in file_paths:\r\n-                loader = UnstructuredFileLoader(datasets_directory)\r\n+                loader = UnstructuredFileLoader(file_path)\r\n                 documents = loader.load()\r\n \r\n                 # Split the text into smaller chunks for better performance\r\n                 text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\r\n"
                },
                {
                    "date": 1678414727162,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -29,9 +29,9 @@\n \r\n                 # Split the text into smaller chunks for better performance\r\n                 text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\r\n                 texts.append(text_splitter.split_documents(documents))\r\n-            print(texts)\r\n+            return texts\r\n         raise PathNotDirectoryException(path=datasets_directory)\r\n \r\n     @staticmethod\r\n     def getFilesInDirectory(directory_path: str) -> list[str]:\r\n"
                },
                {
                    "date": 1678414747016,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -16,22 +16,22 @@\n         # Create an instance of OpenAIEmbeddings using the API \r\n         embeddings = OpenAIEmbeddings(openai_api_key=self._api_key)\r\n         return embeddings\r\n     \r\n-    def createEmbedding(self, datasets_directory: str):\r\n+    def createDocuments(self, datasets_directory: str):\r\n         if os.path.isdir(datasets_directory):\r\n             \r\n-            texts = []\r\n+            documents = []\r\n             file_paths = self.getFilesInDirectory(datasets_directory)\r\n             \r\n             for file_path in file_paths:\r\n                 loader = UnstructuredFileLoader(file_path)\r\n                 documents = loader.load()\r\n \r\n                 # Split the text into smaller chunks for better performance\r\n                 text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\r\n-                texts.append(text_splitter.split_documents(documents))\r\n-            return texts\r\n+                documents.append(text_splitter.split_documents(documents))\r\n+            return documents\r\n         raise PathNotDirectoryException(path=datasets_directory)\r\n \r\n     @staticmethod\r\n     def getFilesInDirectory(directory_path: str) -> list[str]:\r\n"
                },
                {
                    "date": 1678414897265,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -19,19 +19,19 @@\n     \r\n     def createDocuments(self, datasets_directory: str):\r\n         if os.path.isdir(datasets_directory):\r\n             \r\n-            documents = []\r\n+            texts = []\r\n             file_paths = self.getFilesInDirectory(datasets_directory)\r\n             \r\n             for file_path in file_paths:\r\n                 loader = UnstructuredFileLoader(file_path)\r\n                 documents = loader.load()\r\n \r\n                 # Split the text into smaller chunks for better performance\r\n                 text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\r\n-                documents.append(text_splitter.split_documents(documents))\r\n-            return documents\r\n+                texts.append(text_splitter.split_documents(documents))\r\n+            return texts\r\n         raise PathNotDirectoryException(path=datasets_directory)\r\n \r\n     @staticmethod\r\n     def getFilesInDirectory(directory_path: str) -> list[str]:\r\n"
                },
                {
                    "date": 1678414936793,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -16,9 +16,9 @@\n         # Create an instance of OpenAIEmbeddings using the API \r\n         embeddings = OpenAIEmbeddings(openai_api_key=self._api_key)\r\n         return embeddings\r\n     \r\n-    def createDocuments(self, datasets_directory: str):\r\n+    def createTexts(self, datasets_directory: str):\r\n         if os.path.isdir(datasets_directory):\r\n             \r\n             texts = []\r\n             file_paths = self.getFilesInDirectory(datasets_directory)\r\n"
                },
                {
                    "date": 1678415216611,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -28,9 +28,9 @@\n                 documents = loader.load()\r\n \r\n                 # Split the text into smaller chunks for better performance\r\n                 text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\r\n-                texts.append(text_splitter.split_documents(documents))\r\n+                texts.extend(text_splitter.split_documents(documents))\r\n             return texts\r\n         raise PathNotDirectoryException(path=datasets_directory)\r\n \r\n     @staticmethod\r\n"
                },
                {
                    "date": 1678415315684,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -11,9 +11,9 @@\n \r\n     def __init__(self, api_key: str) -> None:\r\n         self._api_key = api_key\r\n \r\n-    def embedding(self):\r\n+    def createEmbedding(self):\r\n         # Create an instance of OpenAIEmbeddings using the API \r\n         embeddings = OpenAIEmbeddings(openai_api_key=self._api_key)\r\n         return embeddings\r\n     \r\n"
                },
                {
                    "date": 1678415461652,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -11,32 +11,36 @@\n \r\n     def __init__(self, api_key: str) -> None:\r\n         self._api_key = api_key\r\n \r\n+    def getDatabase(docs)\r\n+\r\n     def createEmbedding(self):\r\n         # Create an instance of OpenAIEmbeddings using the API \r\n         embeddings = OpenAIEmbeddings(openai_api_key=self._api_key)\r\n         return embeddings\r\n     \r\n-    def createTexts(self, datasets_directory: str):\r\n+    @staticmethod\r\n+    def createDocs(self, datasets_directory: str):\r\n+        def getFilesInDirectory(directory_path: str) -> list[str]:\r\n+            return [os.path.join(directory_path, filename) for filename in os.listdir(directory_path)]\r\n+    \r\n         if os.path.isdir(datasets_directory):\r\n             \r\n-            texts = []\r\n-            file_paths = self.getFilesInDirectory(datasets_directory)\r\n+            docs = []\r\n+            file_paths = getFilesInDirectory(datasets_directory)\r\n             \r\n             for file_path in file_paths:\r\n                 loader = UnstructuredFileLoader(file_path)\r\n                 documents = loader.load()\r\n \r\n                 # Split the text into smaller chunks for better performance\r\n                 text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\r\n-                texts.extend(text_splitter.split_documents(documents))\r\n-            return texts\r\n+                docs.extend(text_splitter.split_documents(documents))\r\n+            return docs\r\n         raise PathNotDirectoryException(path=datasets_directory)\r\n \r\n-    @staticmethod\r\n-    def getFilesInDirectory(directory_path: str) -> list[str]:\r\n-        return [os.path.join(directory_path, filename) for filename in os.listdir(directory_path)]\r\n+\r\n         \r\n # datasets_directory = 'nutrition/datasets/kaggle_food_nutrition'\r\n # persist_directory = 'nutrition/db/kaggle_food_nutrition'\r\n \r\n"
                },
                {
                    "date": 1678415563679,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,5 +1,6 @@\n from langchain.embeddings.openai import OpenAIEmbeddings\r\n+\r\n from langchain.vectorstores import Chroma\r\n from langchain.text_splitter import CharacterTextSplitter\r\n from langchain import OpenAI, VectorDBQA\r\n from langchain.document_loaders import UnstructuredFileLoader\r\n@@ -11,9 +12,10 @@\n \r\n     def __init__(self, api_key: str) -> None:\r\n         self._api_key = api_key\r\n \r\n-    def getDatabase(docs)\r\n+    def getDatabase(self, docs: list, embedding: OpenAIEmbeddings):\r\n+        return\r\n \r\n     def createEmbedding(self):\r\n         # Create an instance of OpenAIEmbeddings using the API \r\n         embeddings = OpenAIEmbeddings(openai_api_key=self._api_key)\r\n"
                },
                {
                    "date": 1678415580996,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -12,10 +12,11 @@\n \r\n     def __init__(self, api_key: str) -> None:\r\n         self._api_key = api_key\r\n \r\n-    def getDatabase(self, docs: list, embedding: OpenAIEmbeddings):\r\n-        return\r\n+    def getDatabase(self, docs: list, embeddings: OpenAIEmbeddings):\r\n+        db = DeepLake.from_documents(docs, embeddings)\r\n+        return db\r\n \r\n     def createEmbedding(self):\r\n         # Create an instance of OpenAIEmbeddings using the API \r\n         embeddings = OpenAIEmbeddings(openai_api_key=self._api_key)\r\n"
                },
                {
                    "date": 1678415596805,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,11 +1,8 @@\n from langchain.embeddings.openai import OpenAIEmbeddings\r\n-\r\n-from langchain.vectorstores import Chroma\r\n from langchain.text_splitter import CharacterTextSplitter\r\n-from langchain import OpenAI, VectorDBQA\r\n-from langchain.document_loaders import UnstructuredFileLoader\r\n-import os\r\n+from langchain.vectorstores import DeepLake\r\n+from langchain.document_loaders import TextLoader\r\n \r\n from exceptions import PathNotDirectoryException\r\n \r\n class LLM:\r\n"
                },
                {
                    "date": 1678415607941,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,8 +1,8 @@\n from langchain.embeddings.openai import OpenAIEmbeddings\r\n from langchain.text_splitter import CharacterTextSplitter\r\n from langchain.vectorstores import DeepLake\r\n-from langchain.document_loaders import TextLoader\r\n+from langchain.document_loaders import UnstructuredFileLoader\r\n \r\n from exceptions import PathNotDirectoryException\r\n \r\n class LLM:\r\n"
                },
                {
                    "date": 1678415629453,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,8 +1,9 @@\n from langchain.embeddings.openai import OpenAIEmbeddings\r\n from langchain.text_splitter import CharacterTextSplitter\r\n from langchain.vectorstores import DeepLake\r\n from langchain.document_loaders import UnstructuredFileLoader\r\n+import os\r\n \r\n from exceptions import PathNotDirectoryException\r\n \r\n class LLM:\r\n"
                },
                {
                    "date": 1678415691720,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -10,8 +10,10 @@\n \r\n     def __init__(self, api_key: str) -> None:\r\n         self._api_key = api_key\r\n \r\n+    def query_database(self, query: str, db: VectorStore)    \r\n+\r\n     def getDatabase(self, docs: list, embeddings: OpenAIEmbeddings):\r\n         db = DeepLake.from_documents(docs, embeddings)\r\n         return db\r\n \r\n"
                },
                {
                    "date": 1678415703928,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,7 +1,8 @@\n from langchain.embeddings.openai import OpenAIEmbeddings\r\n from langchain.text_splitter import CharacterTextSplitter\r\n from langchain.vectorstores import DeepLake\r\n+from langchain.vectorstores import VectorStore\r\n from langchain.document_loaders import UnstructuredFileLoader\r\n import os\r\n \r\n from exceptions import PathNotDirectoryException\r\n@@ -10,9 +11,10 @@\n \r\n     def __init__(self, api_key: str) -> None:\r\n         self._api_key = api_key\r\n \r\n-    def query_database(self, query: str, db: VectorStore)    \r\n+    def query_database(self, query: str, db: VectorStore):\r\n+        return   \r\n \r\n     def getDatabase(self, docs: list, embeddings: OpenAIEmbeddings):\r\n         db = DeepLake.from_documents(docs, embeddings)\r\n         return db\r\n"
                },
                {
                    "date": 1678415711101,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -12,9 +12,9 @@\n     def __init__(self, api_key: str) -> None:\r\n         self._api_key = api_key\r\n \r\n     def query_database(self, query: str, db: VectorStore):\r\n-        return   \r\n+        return docs = db.similarity_search(query)  \r\n \r\n     def getDatabase(self, docs: list, embeddings: OpenAIEmbeddings):\r\n         db = DeepLake.from_documents(docs, embeddings)\r\n         return db\r\n"
                },
                {
                    "date": 1678415716322,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -12,9 +12,9 @@\n     def __init__(self, api_key: str) -> None:\r\n         self._api_key = api_key\r\n \r\n     def query_database(self, query: str, db: VectorStore):\r\n-        return docs = db.similarity_search(query)  \r\n+        return db.similarity_search(query)  \r\n \r\n     def getDatabase(self, docs: list, embeddings: OpenAIEmbeddings):\r\n         db = DeepLake.from_documents(docs, embeddings)\r\n         return db\r\n"
                },
                {
                    "date": 1678415898506,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -24,9 +24,9 @@\n         embeddings = OpenAIEmbeddings(openai_api_key=self._api_key)\r\n         return embeddings\r\n     \r\n     @staticmethod\r\n-    def createDocs(self, datasets_directory: str):\r\n+    def createDocs(datasets_directory: str):\r\n         def getFilesInDirectory(directory_path: str) -> list[str]:\r\n             return [os.path.join(directory_path, filename) for filename in os.listdir(directory_path)]\r\n     \r\n         if os.path.isdir(datasets_directory):\r\n"
                },
                {
                    "date": 1678416785400,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -12,9 +12,9 @@\n     def __init__(self, api_key: str) -> None:\r\n         self._api_key = api_key\r\n \r\n     def query_database(self, query: str, db: VectorStore):\r\n-        return db.similarity_search(query)  \r\n+        return db(query)  \r\n \r\n     def getDatabase(self, docs: list, embeddings: OpenAIEmbeddings):\r\n         db = DeepLake.from_documents(docs, embeddings)\r\n         return db\r\n"
                },
                {
                    "date": 1678417245880,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -12,9 +12,9 @@\n     def __init__(self, api_key: str) -> None:\r\n         self._api_key = api_key\r\n \r\n     def query_database(self, query: str, db: VectorStore):\r\n-        return db(query)  \r\n+        return db.similarity_search(query)  \r\n \r\n     def getDatabase(self, docs: list, embeddings: OpenAIEmbeddings):\r\n         db = DeepLake.from_documents(docs, embeddings)\r\n         return db\r\n@@ -27,9 +27,25 @@\n     @staticmethod\r\n     def createDocs(datasets_directory: str):\r\n         def getFilesInDirectory(directory_path: str) -> list[str]:\r\n             return [os.path.join(directory_path, filename) for filename in os.listdir(directory_path)]\r\n-    \r\n+        \r\n+        def is_csv(infile):\r\n+            try:\r\n+                with open(infile, newline='') as csvfile:\r\n+                    start = csvfile.read(4096)\r\n+\r\n+                    # isprintable does not allow newlines, printable does not allow umlauts...\r\n+                    if not all([c in string.printable or c.isprintable() for c in start]):\r\n+                        return False\r\n+                    dialect = csv.Sniffer().sniff(start)\r\n+                    return True\r\n+            except csv.Error:\r\n+                # Could not get a csv dialect -> probably not a csv.\r\n+                return False\r\n+\r\n+        def getLoader(filepath: str):\r\n+            if is_csv(file_path)\r\n         if os.path.isdir(datasets_directory):\r\n             \r\n             docs = []\r\n             file_paths = getFilesInDirectory(datasets_directory)\r\n"
                },
                {
                    "date": 1678417313050,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -2,8 +2,9 @@\n from langchain.text_splitter import CharacterTextSplitter\r\n from langchain.vectorstores import DeepLake\r\n from langchain.vectorstores import VectorStore\r\n from langchain.document_loaders import UnstructuredFileLoader\r\n+from langchain.document_loaders.csv import CSVLoader\r\n import os\r\n \r\n from exceptions import PathNotDirectoryException\r\n \r\n@@ -28,9 +29,9 @@\n     def createDocs(datasets_directory: str):\r\n         def getFilesInDirectory(directory_path: str) -> list[str]:\r\n             return [os.path.join(directory_path, filename) for filename in os.listdir(directory_path)]\r\n         \r\n-        def is_csv(infile):\r\n+        def is_csv(filepath: str) -> bool:\r\n             try:\r\n                 with open(infile, newline='') as csvfile:\r\n                     start = csvfile.read(4096)\r\n \r\n@@ -43,9 +44,11 @@\n                 # Could not get a csv dialect -> probably not a csv.\r\n                 return False\r\n \r\n         def getLoader(filepath: str):\r\n-            if is_csv(file_path)\r\n+            if is_csv(file_path):\r\n+                return CSVLoader(file_path=file_path)\r\n+            return UnstructuredFileLoader(file_path)\r\n         if os.path.isdir(datasets_directory):\r\n             \r\n             docs = []\r\n             file_paths = getFilesInDirectory(datasets_directory)\r\n"
                },
                {
                    "date": 1678417377333,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -30,20 +30,21 @@\n         def getFilesInDirectory(directory_path: str) -> list[str]:\r\n             return [os.path.join(directory_path, filename) for filename in os.listdir(directory_path)]\r\n         \r\n         def is_csv(filepath: str) -> bool:\r\n+            csv_fileh = open(somefile, 'rb')\r\n             try:\r\n-                with open(infile, newline='') as csvfile:\r\n-                    start = csvfile.read(4096)\r\n+                dialect = csv.Sniffer().sniff(csv_fileh.read(1024))\r\n+                # Perform various checks on the dialect (e.g., lineseparator,\r\n+                # delimiter) to make sure it's sane\r\n \r\n-                    # isprintable does not allow newlines, printable does not allow umlauts...\r\n-                    if not all([c in string.printable or c.isprintable() for c in start]):\r\n-                        return False\r\n-                    dialect = csv.Sniffer().sniff(start)\r\n-                    return True\r\n+                # Don't forget to reset the read position back to the start of\r\n+                # the file before reading any entries.\r\n+                csv_fileh.seek(0)\r\n+                return True\r\n             except csv.Error:\r\n-                # Could not get a csv dialect -> probably not a csv.\r\n-                return False\r\n+                 # File appears not to be in CSV format; move along\r\n+                 return False\r\n \r\n         def getLoader(filepath: str):\r\n             if is_csv(file_path):\r\n                 return CSVLoader(file_path=file_path)\r\n"
                },
                {
                    "date": 1678417402733,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,4 +1,5 @@\n+import csv\r\n from langchain.embeddings.openai import OpenAIEmbeddings\r\n from langchain.text_splitter import CharacterTextSplitter\r\n from langchain.vectorstores import DeepLake\r\n from langchain.vectorstores import VectorStore\r\n@@ -30,17 +31,17 @@\n         def getFilesInDirectory(directory_path: str) -> list[str]:\r\n             return [os.path.join(directory_path, filename) for filename in os.listdir(directory_path)]\r\n         \r\n         def is_csv(filepath: str) -> bool:\r\n-            csv_fileh = open(somefile, 'rb')\r\n+            csv_file = open(filepath, 'rb')\r\n             try:\r\n-                dialect = csv.Sniffer().sniff(csv_fileh.read(1024))\r\n+                dialect = csv.Sniffer().sniff(csv_file.read(1024))\r\n                 # Perform various checks on the dialect (e.g., lineseparator,\r\n                 # delimiter) to make sure it's sane\r\n \r\n                 # Don't forget to reset the read position back to the start of\r\n                 # the file before reading any entries.\r\n-                csv_fileh.seek(0)\r\n+                csv_file.seek(0)\r\n                 return True\r\n             except csv.Error:\r\n                  # File appears not to be in CSV format; move along\r\n                  return False\r\n"
                },
                {
                    "date": 1678417411713,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -49,15 +49,16 @@\n         def getLoader(filepath: str):\r\n             if is_csv(file_path):\r\n                 return CSVLoader(file_path=file_path)\r\n             return UnstructuredFileLoader(file_path)\r\n+        \r\n         if os.path.isdir(datasets_directory):\r\n             \r\n             docs = []\r\n             file_paths = getFilesInDirectory(datasets_directory)\r\n             \r\n             for file_path in file_paths:\r\n-                loader = UnstructuredFileLoader(file_path)\r\n+                loader =getLoader(file_path)\r\n                 documents = loader.load()\r\n \r\n                 # Split the text into smaller chunks for better performance\r\n                 text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\r\n"
                },
                {
                    "date": 1678417423011,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -56,9 +56,9 @@\n             docs = []\r\n             file_paths = getFilesInDirectory(datasets_directory)\r\n             \r\n             for file_path in file_paths:\r\n-                loader =getLoader(file_path)\r\n+                loader = getLoader(file_path)\r\n                 documents = loader.load()\r\n \r\n                 # Split the text into smaller chunks for better performance\r\n                 text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\r\n"
                },
                {
                    "date": 1678417470271,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -3,9 +3,8 @@\n from langchain.text_splitter import CharacterTextSplitter\r\n from langchain.vectorstores import DeepLake\r\n from langchain.vectorstores import VectorStore\r\n from langchain.document_loaders import UnstructuredFileLoader\r\n-from langchain.document_loaders.csv import CSVLoader\r\n import os\r\n \r\n from exceptions import PathNotDirectoryException\r\n \r\n"
                },
                {
                    "date": 1678417482321,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -3,8 +3,9 @@\n from langchain.text_splitter import CharacterTextSplitter\r\n from langchain.vectorstores import DeepLake\r\n from langchain.vectorstores import VectorStore\r\n from langchain.document_loaders import UnstructuredFileLoader\r\n+from langchain.document_loaders.csv import CSVLoader\r\n import os\r\n \r\n from exceptions import PathNotDirectoryException\r\n \r\n"
                },
                {
                    "date": 1678417710001,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -3,9 +3,9 @@\n from langchain.text_splitter import CharacterTextSplitter\r\n from langchain.vectorstores import DeepLake\r\n from langchain.vectorstores import VectorStore\r\n from langchain.document_loaders import UnstructuredFileLoader\r\n-from langchain.document_loaders.csv import CSVLoader\r\n+from langchain.document_loaders import CSVLoader\r\n import os\r\n \r\n from exceptions import PathNotDirectoryException\r\n \r\n"
                },
                {
                    "date": 1678417818039,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -48,9 +48,9 @@\n \r\n         def getLoader(filepath: str):\r\n             if is_csv(file_path):\r\n                 return CSVLoader(file_path=file_path)\r\n-            return UnstructuredFileLoader(file_path)\r\n+            return UnstructuredFileLoader(file_path, mode=\"elements\")\r\n         \r\n         if os.path.isdir(datasets_directory):\r\n             \r\n             docs = []\r\n"
                },
                {
                    "date": 1678417830350,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -30,35 +30,15 @@\n     def createDocs(datasets_directory: str):\r\n         def getFilesInDirectory(directory_path: str) -> list[str]:\r\n             return [os.path.join(directory_path, filename) for filename in os.listdir(directory_path)]\r\n         \r\n-        def is_csv(filepath: str) -> bool:\r\n-            csv_file = open(filepath, 'rb')\r\n-            try:\r\n-                dialect = csv.Sniffer().sniff(csv_file.read(1024))\r\n-                # Perform various checks on the dialect (e.g., lineseparator,\r\n-                # delimiter) to make sure it's sane\r\n-\r\n-                # Don't forget to reset the read position back to the start of\r\n-                # the file before reading any entries.\r\n-                csv_file.seek(0)\r\n-                return True\r\n-            except csv.Error:\r\n-                 # File appears not to be in CSV format; move along\r\n-                 return False\r\n-\r\n-        def getLoader(filepath: str):\r\n-            if is_csv(file_path):\r\n-                return CSVLoader(file_path=file_path)\r\n-            return UnstructuredFileLoader(file_path, mode=\"elements\")\r\n-        \r\n         if os.path.isdir(datasets_directory):\r\n             \r\n             docs = []\r\n             file_paths = getFilesInDirectory(datasets_directory)\r\n             \r\n             for file_path in file_paths:\r\n-                loader = getLoader(file_path)\r\n+                loader = UnstructuredFileLoader(file_path, mode=\"elements\")\r\n                 documents = loader.load()\r\n \r\n                 # Split the text into smaller chunks for better performance\r\n                 text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\r\n"
                },
                {
                    "date": 1678417991762,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -3,9 +3,9 @@\n from langchain.text_splitter import CharacterTextSplitter\r\n from langchain.vectorstores import DeepLake\r\n from langchain.vectorstores import VectorStore\r\n from langchain.document_loaders import UnstructuredFileLoader\r\n-from langchain.document_loaders import CSVLoader\r\n+from langchain.document_loaders.csv import CSVLoader\r\n import os\r\n \r\n from exceptions import PathNotDirectoryException\r\n \r\n"
                },
                {
                    "date": 1678419813978,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -3,9 +3,8 @@\n from langchain.text_splitter import CharacterTextSplitter\r\n from langchain.vectorstores import DeepLake\r\n from langchain.vectorstores import VectorStore\r\n from langchain.document_loaders import UnstructuredFileLoader\r\n-from langchain.document_loaders.csv import CSVLoader\r\n import os\r\n \r\n from exceptions import PathNotDirectoryException\r\n \r\n"
                },
                {
                    "date": 1678420942665,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -12,9 +12,9 @@\n \r\n     def __init__(self, api_key: str) -> None:\r\n         self._api_key = api_key\r\n \r\n-    def query_database(self, query: str, db: VectorStore):\r\n+    def query_database(self, query: str, db: VectorStore, k: int):\r\n         return db.similarity_search(query)  \r\n \r\n     def getDatabase(self, docs: list, embeddings: OpenAIEmbeddings):\r\n         db = DeepLake.from_documents(docs, embeddings)\r\n"
                },
                {
                    "date": 1678420960871,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -12,10 +12,10 @@\n \r\n     def __init__(self, api_key: str) -> None:\r\n         self._api_key = api_key\r\n \r\n-    def query_database(self, query: str, db: VectorStore, k: int):\r\n-        return db.similarity_search(query)  \r\n+    def query_database(self, query: str, db: VectorStore, k: int = 4):\r\n+        return db.similarity_search(query, k=k)  \r\n \r\n     def getDatabase(self, docs: list, embeddings: OpenAIEmbeddings):\r\n         db = DeepLake.from_documents(docs, embeddings)\r\n         return db\r\n"
                },
                {
                    "date": 1678421362257,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -15,9 +15,9 @@\n \r\n     def query_database(self, query: str, db: VectorStore, k: int = 4):\r\n         return db.similarity_search(query, k=k)  \r\n \r\n-    def getDatabase(self, docs: list, embeddings: OpenAIEmbeddings):\r\n+    def getDatabase(self, docs: list, embeddings: OpenAIEmbeddings, db_dir: str = ''):\r\n         db = DeepLake.from_documents(docs, embeddings)\r\n         return db\r\n \r\n     def createEmbedding(self):\r\n"
                },
                {
                    "date": 1678421386790,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -16,9 +16,10 @@\n     def query_database(self, query: str, db: VectorStore, k: int = 4):\r\n         return db.similarity_search(query, k=k)  \r\n \r\n     def getDatabase(self, docs: list, embeddings: OpenAIEmbeddings, db_dir: str = ''):\r\n-        db = DeepLake.from_documents(docs, embeddings)\r\n+        if len(db_dir) == 0:\r\n+            db = DeepLake.from_documents(docs, embeddings)\r\n         return db\r\n \r\n     def createEmbedding(self):\r\n         # Create an instance of OpenAIEmbeddings using the API \r\n"
                },
                {
                    "date": 1678421436724,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -16,9 +16,9 @@\n     def query_database(self, query: str, db: VectorStore, k: int = 4):\r\n         return db.similarity_search(query, k=k)  \r\n \r\n     def getDatabase(self, docs: list, embeddings: OpenAIEmbeddings, db_dir: str = ''):\r\n-        if len(db_dir) == 0:\r\n+        if len(db_dir) == 0 or not os.path.exists(persist_directory):\r\n             db = DeepLake.from_documents(docs, embeddings)\r\n         return db\r\n \r\n     def createEmbedding(self):\r\n"
                },
                {
                    "date": 1678421492210,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -15,11 +15,13 @@\n \r\n     def query_database(self, query: str, db: VectorStore, k: int = 4):\r\n         return db.similarity_search(query, k=k)  \r\n \r\n-    def getDatabase(self, docs: list, embeddings: OpenAIEmbeddings, db_dir: str = ''):\r\n-        if len(db_dir) == 0 or not os.path.exists(persist_directory):\r\n-            db = DeepLake.from_documents(docs, embeddings)\r\n+    def getDatabase(self, docs: list, embedding: OpenAIEmbeddings, db_dir: str = ''):\r\n+        if len(db_dir) == 0 or not os.path.exists(db_dir):\r\n+            db = DeepLake.from_documents(docs, embedding)\r\n+        else:\r\n+            db = DeepLake.from_documents(documents=docs, embedding=embedding, dataset_path=dataset_path)\r\n         return db\r\n \r\n     def createEmbedding(self):\r\n         # Create an instance of OpenAIEmbeddings using the API \r\n"
                },
                {
                    "date": 1678421515313,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -17,11 +17,11 @@\n         return db.similarity_search(query, k=k)  \r\n \r\n     def getDatabase(self, docs: list, embedding: OpenAIEmbeddings, db_dir: str = ''):\r\n         if len(db_dir) == 0 or not os.path.exists(db_dir):\r\n-            db = DeepLake.from_documents(docs, embedding)\r\n+            db = DeepLake.from_documents(documents=docs, embedding=embedding)\r\n         else:\r\n-            db = DeepLake.from_documents(documents=docs, embedding=embedding, dataset_path=dataset_path)\r\n+            db = DeepLake.from_documents(documents=docs, embedding=embedding, dataset_path=db_dir)\r\n         return db\r\n \r\n     def createEmbedding(self):\r\n         # Create an instance of OpenAIEmbeddings using the API \r\n"
                },
                {
                    "date": 1678421525202,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -7,55 +7,56 @@\n import os\r\n \r\n from exceptions import PathNotDirectoryException\r\n \r\n+\r\n class LLM:\r\n \r\n     def __init__(self, api_key: str) -> None:\r\n         self._api_key = api_key\r\n \r\n     def query_database(self, query: str, db: VectorStore, k: int = 4):\r\n-        return db.similarity_search(query, k=k)  \r\n+        return db.similarity_search(query, k=k)\r\n \r\n     def getDatabase(self, docs: list, embedding: OpenAIEmbeddings, db_dir: str = ''):\r\n         if len(db_dir) == 0 or not os.path.exists(db_dir):\r\n             db = DeepLake.from_documents(documents=docs, embedding=embedding)\r\n         else:\r\n-            db = DeepLake.from_documents(documents=docs, embedding=embedding, dataset_path=db_dir)\r\n+            db = DeepLake.from_documents(\r\n+                documents=docs, embedding=embedding, dataset_path=db_dir)\r\n         return db\r\n \r\n     def createEmbedding(self):\r\n-        # Create an instance of OpenAIEmbeddings using the API \r\n+        # Create an instance of OpenAIEmbeddings using the API\r\n         embeddings = OpenAIEmbeddings(openai_api_key=self._api_key)\r\n         return embeddings\r\n-    \r\n+\r\n     @staticmethod\r\n     def createDocs(datasets_directory: str):\r\n         def getFilesInDirectory(directory_path: str) -> list[str]:\r\n             return [os.path.join(directory_path, filename) for filename in os.listdir(directory_path)]\r\n-        \r\n+\r\n         if os.path.isdir(datasets_directory):\r\n-            \r\n+\r\n             docs = []\r\n             file_paths = getFilesInDirectory(datasets_directory)\r\n-            \r\n+\r\n             for file_path in file_paths:\r\n                 loader = UnstructuredFileLoader(file_path, mode=\"elements\")\r\n                 documents = loader.load()\r\n \r\n                 # Split the text into smaller chunks for better performance\r\n-                text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\r\n+                text_splitter = CharacterTextSplitter(\r\n+                    chunk_size=1000, chunk_overlap=0)\r\n                 docs.extend(text_splitter.split_documents(documents))\r\n             return docs\r\n         raise PathNotDirectoryException(path=datasets_directory)\r\n \r\n \r\n-        \r\n # datasets_directory = 'nutrition/datasets/kaggle_food_nutrition'\r\n # persist_directory = 'nutrition/db/kaggle_food_nutrition'\r\n \r\n \r\n-\r\n # # If the vector database has already been created, load it from disk\r\n # if os.path.exists(persist_directory):\r\n #     vectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\r\n # else:\r\n@@ -77,9 +78,4 @@\n # query = \"200g of lean beef how many calories\"\r\n # result = qa({\"query\": query})\r\n # print(result['result'])\r\n # print(result['source_documents'])\r\n-\r\n-\r\n-\r\n-    \r\n-\r\n"
                },
                {
                    "date": 1678421572488,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -22,8 +22,10 @@\n             db = DeepLake.from_documents(documents=docs, embedding=embedding)\r\n         else:\r\n             db = DeepLake.from_documents(\r\n                 documents=docs, embedding=embedding, dataset_path=db_dir)\r\n+        db = DeepLake.from_documents(\r\n+                documents=docs, embedding=embedding, dataset_path=db_dir)\r\n         return db\r\n \r\n     def createEmbedding(self):\r\n         # Create an instance of OpenAIEmbeddings using the API\r\n"
                },
                {
                    "date": 1678421643373,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -17,13 +17,13 @@\n     def query_database(self, query: str, db: VectorStore, k: int = 4):\r\n         return db.similarity_search(query, k=k)\r\n \r\n     def getDatabase(self, docs: list, embedding: OpenAIEmbeddings, db_dir: str = ''):\r\n-        if len(db_dir) == 0 or not os.path.exists(db_dir):\r\n-            db = DeepLake.from_documents(documents=docs, embedding=embedding)\r\n-        else:\r\n-            db = DeepLake.from_documents(\r\n-                documents=docs, embedding=embedding, dataset_path=db_dir)\r\n+        # if len(db_dir) == 0 or not os.path.exists(db_dir):\r\n+        #     db = DeepLake.from_documents(documents=docs, embedding=embedding)\r\n+        # else:\r\n+        #     db = DeepLake.from_documents(\r\n+        #         documents=docs, embedding=embedding, dataset_path=db_dir)\r\n         db = DeepLake.from_documents(\r\n                 documents=docs, embedding=embedding, dataset_path=db_dir)\r\n         return db\r\n \r\n"
                },
                {
                    "date": 1678421899337,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -17,13 +17,13 @@\n     def query_database(self, query: str, db: VectorStore, k: int = 4):\r\n         return db.similarity_search(query, k=k)\r\n \r\n     def getDatabase(self, docs: list, embedding: OpenAIEmbeddings, db_dir: str = ''):\r\n-        # if len(db_dir) == 0 or not os.path.exists(db_dir):\r\n-        #     db = DeepLake.from_documents(documents=docs, embedding=embedding)\r\n-        # else:\r\n-        #     db = DeepLake.from_documents(\r\n-        #         documents=docs, embedding=embedding, dataset_path=db_dir)\r\n+        if len(db_dir) == 0 or not os.path.exists(db_dir):\r\n+            db = DeepLake.from_documents(documents=docs, embedding=embedding)\r\n+        else:\r\n+            db = DeepLake.from_documents(\r\n+                documents=docs, embedding=embedding, dataset_path=db_dir)\r\n         db = DeepLake.from_documents(\r\n                 documents=docs, embedding=embedding, dataset_path=db_dir)\r\n         return db\r\n \r\n"
                },
                {
                    "date": 1678421922575,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -17,9 +17,9 @@\n     def query_database(self, query: str, db: VectorStore, k: int = 4):\r\n         return db.similarity_search(query, k=k)\r\n \r\n     def getDatabase(self, docs: list, embedding: OpenAIEmbeddings, db_dir: str = ''):\r\n-        if len(db_dir) == 0 or not os.path.exists(db_dir):\r\n+        if len(db_dir) == 0:\r\n             db = DeepLake.from_documents(documents=docs, embedding=embedding)\r\n         else:\r\n             db = DeepLake.from_documents(\r\n                 documents=docs, embedding=embedding, dataset_path=db_dir)\r\n"
                },
                {
                    "date": 1678421937044,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -16,14 +16,9 @@\n \r\n     def query_database(self, query: str, db: VectorStore, k: int = 4):\r\n         return db.similarity_search(query, k=k)\r\n \r\n-    def getDatabase(self, docs: list, embedding: OpenAIEmbeddings, db_dir: str = ''):\r\n-        if len(db_dir) == 0:\r\n-            db = DeepLake.from_documents(documents=docs, embedding=embedding)\r\n-        else:\r\n-            db = DeepLake.from_documents(\r\n-                documents=docs, embedding=embedding, dataset_path=db_dir)\r\n+    def getDatabase(self, docs: list, embedding: OpenAIEmbeddings, db_dir: str):\r\n         db = DeepLake.from_documents(\r\n                 documents=docs, embedding=embedding, dataset_path=db_dir)\r\n         return db\r\n \r\n"
                },
                {
                    "date": 1678422560382,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -17,8 +17,9 @@\n     def query_database(self, query: str, db: VectorStore, k: int = 4):\r\n         return db.similarity_search(query, k=k)\r\n \r\n     def getDatabase(self, docs: list, embedding: OpenAIEmbeddings, db_dir: str):\r\n+        db = DeepLake(dataset_path=db_dir, embedding_function= embedding)\r\n         db = DeepLake.from_documents(\r\n                 documents=docs, embedding=embedding, dataset_path=db_dir)\r\n         return db\r\n \r\n"
                },
                {
                    "date": 1678422596602,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -17,10 +17,12 @@\n     def query_database(self, query: str, db: VectorStore, k: int = 4):\r\n         return db.similarity_search(query, k=k)\r\n \r\n     def getDatabase(self, docs: list, embedding: OpenAIEmbeddings, db_dir: str):\r\n-        db = DeepLake(dataset_path=db_dir, embedding_function= embedding)\r\n-        db = DeepLake.from_documents(\r\n+        if os.path.exists(db_dir):\r\n+            db = DeepLake(dataset_path=db_dir, embedding_function= embedding)\r\n+        else:\r\n+            db = DeepLake.from_documents(\r\n                 documents=docs, embedding=embedding, dataset_path=db_dir)\r\n         return db\r\n \r\n     def createEmbedding(self):\r\n"
                },
                {
                    "date": 1678422645056,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -18,14 +18,19 @@\n         return db.similarity_search(query, k=k)\r\n \r\n     def getDatabase(self, docs: list, embedding: OpenAIEmbeddings, db_dir: str):\r\n         if os.path.exists(db_dir):\r\n-            db = DeepLake(dataset_path=db_dir, embedding_function= embedding)\r\n+            db = DeepLake(dataset_path=db_dir, embedding_function=embedding)\r\n         else:\r\n             db = DeepLake.from_documents(\r\n                 documents=docs, embedding=embedding, dataset_path=db_dir)\r\n         return db\r\n-\r\n+    \r\n+    def createDatabase(self, docs: list, embedding: OpenAIEmbeddings, db_dir: str):\r\n+        db = DeepLake.from_documents(\r\n+                documents=docs, embedding=embedding, dataset_path=db_dir)\r\n+        return db\r\n+    \r\n     def createEmbedding(self):\r\n         # Create an instance of OpenAIEmbeddings using the API\r\n         embeddings = OpenAIEmbeddings(openai_api_key=self._api_key)\r\n         return embeddings\r\n"
                },
                {
                    "date": 1678422658323,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -16,14 +16,10 @@\n \r\n     def query_database(self, query: str, db: VectorStore, k: int = 4):\r\n         return db.similarity_search(query, k=k)\r\n \r\n-    def getDatabase(self, docs: list, embedding: OpenAIEmbeddings, db_dir: str):\r\n-        if os.path.exists(db_dir):\r\n+    def getDatabase(self, embedding: OpenAIEmbeddings, db_dir: str):\r\n             db = DeepLake(dataset_path=db_dir, embedding_function=embedding)\r\n-        else:\r\n-            db = DeepLake.from_documents(\r\n-                documents=docs, embedding=embedding, dataset_path=db_dir)\r\n         return db\r\n     \r\n     def createDatabase(self, docs: list, embedding: OpenAIEmbeddings, db_dir: str):\r\n         db = DeepLake.from_documents(\r\n"
                },
                {
                    "date": 1678422664369,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -17,9 +17,9 @@\n     def query_database(self, query: str, db: VectorStore, k: int = 4):\r\n         return db.similarity_search(query, k=k)\r\n \r\n     def getDatabase(self, embedding: OpenAIEmbeddings, db_dir: str):\r\n-            db = DeepLake(dataset_path=db_dir, embedding_function=embedding)\r\n+        db = DeepLake(dataset_path=db_dir, embedding_function=embedding)\r\n         return db\r\n     \r\n     def createDatabase(self, docs: list, embedding: OpenAIEmbeddings, db_dir: str):\r\n         db = DeepLake.from_documents(\r\n"
                },
                {
                    "date": 1678424316465,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -13,9 +13,9 @@\n \r\n     def __init__(self, api_key: str) -> None:\r\n         self._api_key = api_key\r\n \r\n-    def query_database(self, query: str, db: VectorStore, k: int = 4):\r\n+    def query_database(self, query: str, db, k: int = 4):\r\n         return db.similarity_search(query, k=k)\r\n \r\n     def getDatabase(self, embedding: OpenAIEmbeddings, db_dir: str):\r\n         db = DeepLake(dataset_path=db_dir, embedding_function=embedding)\r\n"
                },
                {
                    "date": 1678424330762,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -13,9 +13,9 @@\n \r\n     def __init__(self, api_key: str) -> None:\r\n         self._api_key = api_key\r\n \r\n-    def query_database(self, query: str, db, k: int = 4):\r\n+    def query_database(self, query: str, db: VectorStore | DataLake, k: int = 4):\r\n         return db.similarity_search(query, k=k)\r\n \r\n     def getDatabase(self, embedding: OpenAIEmbeddings, db_dir: str):\r\n         db = DeepLake(dataset_path=db_dir, embedding_function=embedding)\r\n"
                },
                {
                    "date": 1678424339842,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -13,9 +13,9 @@\n \r\n     def __init__(self, api_key: str) -> None:\r\n         self._api_key = api_key\r\n \r\n-    def query_database(self, query: str, db: VectorStore | DataLake, k: int = 4):\r\n+    def query_database(self, query: str, db: VectorStore | DeepLake, k: int = 4):\r\n         return db.similarity_search(query, k=k)\r\n \r\n     def getDatabase(self, embedding: OpenAIEmbeddings, db_dir: str):\r\n         db = DeepLake(dataset_path=db_dir, embedding_function=embedding)\r\n"
                },
                {
                    "date": 1678425398276,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -49,34 +49,5 @@\n                 text_splitter = CharacterTextSplitter(\r\n                     chunk_size=1000, chunk_overlap=0)\r\n                 docs.extend(text_splitter.split_documents(documents))\r\n             return docs\r\n-        raise PathNotDirectoryException(path=datasets_directory)\r\n-\r\n-\r\n-# datasets_directory = 'nutrition/datasets/kaggle_food_nutrition'\r\n-# persist_directory = 'nutrition/db/kaggle_food_nutrition'\r\n-\r\n-\r\n-# # If the vector database has already been created, load it from disk\r\n-# if os.path.exists(persist_directory):\r\n-#     vectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\r\n-# else:\r\n-#     # If the vector database hasn't been created, load the dataset from disk\r\n-#     loader = DirectoryLoader(datasets_directory, glob='**/*.txt')\r\n-#     documents = loader.load()\r\n-\r\n-#     # Split the text into smaller chunks for better performance\r\n-#     text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\r\n-#     texts = text_splitter.split_documents(documents)\r\n-\r\n-#     # Create the vector database from the documents\r\n-#     vectordb = Chroma.from_documents(documents=texts, embedding=embeddings, persist_directory=persist_directory)\r\n-\r\n-# # Create a VectorDBQA instance for answering questions\r\n-# qa = VectorDBQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", vectorstore=vectordb , return_source_documents=True)\r\n-\r\n-# # Ask a question and print the result and source documents\r\n-# query = \"200g of lean beef how many calories\"\r\n-# result = qa({\"query\": query})\r\n-# print(result['result'])\r\n-# print(result['source_documents'])\r\n+        raise PathNotDirectoryException(path=datasets_directory)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1678484054961,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -8,9 +8,9 @@\n \r\n from exceptions import PathNotDirectoryException\r\n \r\n \r\n-class LLM:\r\n+class Recommender:\r\n \r\n     def __init__(self, api_key: str) -> None:\r\n         self._api_key = api_key\r\n \r\n"
                }
            ],
            "date": 1678414140726,
            "name": "Commit-0",
            "content": "from langchain.embeddings.openai import OpenAIEmbeddings\r\nfrom langchain.vectorstores import Chroma\r\nfrom langchain.text_splitter import CharacterTextSplitter\r\nfrom langchain import OpenAI, VectorDBQA\r\nfrom langchain.document_loaders import UnstructuredFileLoader\r\nimport os\r\n\r\nfrom exceptions import PathNotDirectoryException\r\n\r\nclass LLM:\r\n\r\n    def __init__(self, api_key: str) -> None:\r\n        self._api_key = api_key\r\n\r\n    def embedding(self):\r\n        # Create an instance of OpenAIEmbeddings using the API \r\n        embeddings = OpenAIEmbeddings(openai_api_key=self._api_key)\r\n        return embeddings\r\n    \r\n    def createEmbedding(self, datasets_directory: str):\r\n        if os.path.isdir(datasets_directory):\r\n            loader = UnstructuredFileLoader(datasets_directory)\r\n            documents = loader.load()\r\n\r\n            # Split the text into smaller chunks for better performance\r\n            text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\r\n            texts = text_splitter.split_documents(documents)\r\n            return texts\r\n        raise PathNotDirectoryException\r\n\r\n# # Set the directories for the dataset and the persisted vector database\r\n# datasets_directory = 'nutrition/datasets/kaggle_food_nutrition'\r\n# persist_directory = 'nutrition/db/kaggle_food_nutrition'\r\n\r\n\r\n\r\n# # If the vector database has already been created, load it from disk\r\n# if os.path.exists(persist_directory):\r\n#     vectordb = Chroma(persist_directory=persist_directory, embedding_function=embeddings)\r\n# else:\r\n#     # If the vector database hasn't been created, load the dataset from disk\r\n#     loader = DirectoryLoader(datasets_directory, glob='**/*.txt')\r\n#     documents = loader.load()\r\n\r\n#     # Split the text into smaller chunks for better performance\r\n#     text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\r\n#     texts = text_splitter.split_documents(documents)\r\n\r\n#     # Create the vector database from the documents\r\n#     vectordb = Chroma.from_documents(documents=texts, embedding=embeddings, persist_directory=persist_directory)\r\n\r\n# # Create a VectorDBQA instance for answering questions\r\n# qa = VectorDBQA.from_chain_type(llm=OpenAI(), chain_type=\"stuff\", vectorstore=vectordb , return_source_documents=True)\r\n\r\n# # Ask a question and print the result and source documents\r\n# query = \"200g of lean beef how many calories\"\r\n# result = qa({\"query\": query})\r\n# print(result['result'])\r\n# print(result['source_documents'])\r\n\r\n\r\n\r\n    \r\n\r\n"
        }
    ]
}