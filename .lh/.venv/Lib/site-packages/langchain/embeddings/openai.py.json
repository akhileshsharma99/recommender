{
    "sourceFile": ".venv/Lib/site-packages/langchain/embeddings/openai.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1678413998195,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1678413998195,
            "name": "Commit-0",
            "content": "\"\"\"Wrapper around OpenAI embedding models.\"\"\"\nfrom __future__ import annotations\n\nimport logging\nfrom typing import Any, Callable, Dict, List, Optional\n\nimport numpy as np\nfrom pydantic import BaseModel, Extra, root_validator\nfrom tenacity import (\n    before_sleep_log,\n    retry,\n    retry_if_exception_type,\n    stop_after_attempt,\n    wait_exponential,\n)\n\nfrom langchain.embeddings.base import Embeddings\nfrom langchain.utils import get_from_dict_or_env\n\nlogger = logging.getLogger(__name__)\n\n\ndef _create_retry_decorator(embeddings: OpenAIEmbeddings) -> Callable[[Any], Any]:\n    import openai\n\n    min_seconds = 4\n    max_seconds = 10\n    # Wait 2^x * 1 second between each retry starting with\n    # 4 seconds, then up to 10 seconds, then 10 seconds afterwards\n    return retry(\n        reraise=True,\n        stop=stop_after_attempt(embeddings.max_retries),\n        wait=wait_exponential(multiplier=1, min=min_seconds, max=max_seconds),\n        retry=(\n            retry_if_exception_type(openai.error.Timeout)\n            | retry_if_exception_type(openai.error.APIError)\n            | retry_if_exception_type(openai.error.APIConnectionError)\n            | retry_if_exception_type(openai.error.RateLimitError)\n            | retry_if_exception_type(openai.error.ServiceUnavailableError)\n        ),\n        before_sleep=before_sleep_log(logger, logging.WARNING),\n    )\n\n\ndef embed_with_retry(embeddings: OpenAIEmbeddings, **kwargs: Any) -> Any:\n    \"\"\"Use tenacity to retry the completion call.\"\"\"\n    retry_decorator = _create_retry_decorator(embeddings)\n\n    @retry_decorator\n    def _completion_with_retry(**kwargs: Any) -> Any:\n        return embeddings.client.create(**kwargs)\n\n    return _completion_with_retry(**kwargs)\n\n\nclass OpenAIEmbeddings(BaseModel, Embeddings):\n    \"\"\"Wrapper around OpenAI embedding models.\n\n    To use, you should have the ``openai`` python package installed, and the\n    environment variable ``OPENAI_API_KEY`` set with your API key or pass it\n    as a named parameter to the constructor.\n\n    Example:\n        .. code-block:: python\n\n            from langchain.embeddings import OpenAIEmbeddings\n            openai = OpenAIEmbeddings(openai_api_key=\"my-api-key\")\n    \"\"\"\n\n    client: Any  #: :meta private:\n    document_model_name: str = \"text-embedding-ada-002\"\n    query_model_name: str = \"text-embedding-ada-002\"\n    embedding_ctx_length: int = -1\n    openai_api_key: Optional[str] = None\n    chunk_size: int = 1000\n    \"\"\"Maximum number of texts to embed in each batch\"\"\"\n    max_retries: int = 6\n    \"\"\"Maximum number of retries to make when generating.\"\"\"\n\n    class Config:\n        \"\"\"Configuration for this pydantic object.\"\"\"\n\n        extra = Extra.forbid\n\n    # TODO: deprecate this\n    @root_validator(pre=True)\n    def get_model_names(cls, values: Dict) -> Dict:\n        \"\"\"Get model names from just old model name.\"\"\"\n        if \"model_name\" in values:\n            if \"document_model_name\" in values:\n                raise ValueError(\n                    \"Both `model_name` and `document_model_name` were provided, \"\n                    \"but only one should be.\"\n                )\n            if \"query_model_name\" in values:\n                raise ValueError(\n                    \"Both `model_name` and `query_model_name` were provided, \"\n                    \"but only one should be.\"\n                )\n            model_name = values.pop(\"model_name\")\n            values[\"document_model_name\"] = f\"text-search-{model_name}-doc-001\"\n            values[\"query_model_name\"] = f\"text-search-{model_name}-query-001\"\n        return values\n\n    @root_validator()\n    def validate_environment(cls, values: Dict) -> Dict:\n        \"\"\"Validate that api key and python package exists in environment.\"\"\"\n        openai_api_key = get_from_dict_or_env(\n            values, \"openai_api_key\", \"OPENAI_API_KEY\"\n        )\n        try:\n            import openai\n\n            openai.api_key = openai_api_key\n            values[\"client\"] = openai.Embedding\n        except ImportError:\n            raise ValueError(\n                \"Could not import openai python package. \"\n                \"Please it install it with `pip install openai`.\"\n            )\n        return values\n\n    # please refer to\n    # https://github.com/openai/openai-cookbook/blob/main/examples/Embedding_long_inputs.ipynb\n    def _get_len_safe_embeddings(\n        self, texts: List[str], *, engine: str, chunk_size: Optional[int] = None\n    ) -> List[List[float]]:\n        embeddings: List[List[float]] = [[] for i in range(len(texts))]\n        try:\n            import tiktoken\n\n            tokens = []\n            indices = []\n            encoding = tiktoken.model.encoding_for_model(self.document_model_name)\n            for i, text in enumerate(texts):\n                # replace newlines, which can negatively affect performance.\n                text = text.replace(\"\\n\", \" \")\n                token = encoding.encode(text)\n                for j in range(0, len(token), self.embedding_ctx_length):\n                    tokens += [token[j : j + self.embedding_ctx_length]]\n                    indices += [i]\n\n            batched_embeddings = []\n            _chunk_size = chunk_size or self.chunk_size\n            for i in range(0, len(tokens), _chunk_size):\n                response = embed_with_retry(\n                    self,\n                    input=tokens[i : i + _chunk_size],\n                    engine=self.document_model_name,\n                )\n                batched_embeddings += [r[\"embedding\"] for r in response[\"data\"]]\n\n            results: List[List[List[float]]] = [[] for i in range(len(texts))]\n            lens: List[List[int]] = [[] for i in range(len(texts))]\n            for i in range(len(indices)):\n                results[indices[i]].append(batched_embeddings[i])\n                lens[indices[i]].append(len(batched_embeddings[i]))\n\n            for i in range(len(texts)):\n                average = np.average(results[i], axis=0, weights=lens[i])\n                embeddings[i] = (average / np.linalg.norm(average)).tolist()\n\n            return embeddings\n\n        except ImportError:\n            raise ValueError(\n                \"Could not import tiktoken python package. \"\n                \"This is needed in order to for OpenAIEmbeddings. \"\n                \"Please it install it with `pip install tiktoken`.\"\n            )\n\n    def _embedding_func(self, text: str, *, engine: str) -> List[float]:\n        \"\"\"Call out to OpenAI's embedding endpoint.\"\"\"\n        # replace newlines, which can negatively affect performance.\n        if self.embedding_ctx_length > 0:\n            return self._get_len_safe_embeddings([text], engine=engine)[0]\n        else:\n            text = text.replace(\"\\n\", \" \")\n            return embed_with_retry(self, input=[text], engine=engine)[\"data\"][0][\n                \"embedding\"\n            ]\n\n    def embed_documents(\n        self, texts: List[str], chunk_size: Optional[int] = 0\n    ) -> List[List[float]]:\n        \"\"\"Call out to OpenAI's embedding endpoint for embedding search docs.\n\n        Args:\n            texts: The list of texts to embed.\n            chunk_size: The chunk size of embeddings. If None, will use the chunk size\n                specified by the class.\n\n        Returns:\n            List of embeddings, one for each text.\n        \"\"\"\n        # handle large batches of texts\n        if self.embedding_ctx_length > 0:\n            return self._get_len_safe_embeddings(texts, engine=self.document_model_name)\n        else:\n            results = []\n            _chunk_size = chunk_size or self.chunk_size\n            for i in range(0, len(texts), _chunk_size):\n                response = embed_with_retry(\n                    self,\n                    input=texts[i : i + _chunk_size],\n                    engine=self.document_model_name,\n                )\n                results += [r[\"embedding\"] for r in response[\"data\"]]\n            return results\n\n    def embed_query(self, text: str) -> List[float]:\n        \"\"\"Call out to OpenAI's embedding endpoint for embedding query text.\n\n        Args:\n            text: The text to embed.\n\n        Returns:\n            Embeddings for the text.\n        \"\"\"\n        embedding = self._embedding_func(text, engine=self.query_model_name)\n        return embedding\n"
        }
    ]
}